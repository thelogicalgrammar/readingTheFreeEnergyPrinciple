{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A close, close reading of Buckley et al. (2017)\n",
    "\n",
    "\n",
    "## 1. Introduction and 2. An overview of FEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $-\\ln( p ( \\phi ) )$ is the surprisal of the _real_ distribution over sensory states. That is one of the reasons that free energy turns out to be so important: it gives us information about the real distribution over sensory states.\n",
    "\n",
    "- R-density: density over world states.\n",
    "- G-density: joint density of world states and sensory input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variational free energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\{ \\vartheta \\}$: environmental states\n",
    "- $\\{ \\varphi \\}$: sensory inputs\n",
    "\n",
    "- Equation (1)\n",
    "$$\n",
    "p(\\vartheta, \\varphi) = p(\\varphi | \\vartheta) p(\\vartheta) \n",
    "$$\n",
    "is an instance of the _chain rule_ of probability.\n",
    "\n",
    "- All integrals are actually over the domain of the distribution, which is assumed to be $(-\\infty, \\infty)$. Moreover, they write the $dx$ right after the integral sign (because they are physicists I assume?), but I'll write it as the end because it functions like a closed parenthesis. So I'll write $\\int_D f(x) dx$ where they write $\\int f(x) dx$, unless they mean an indefinite integral.\n",
    "- Equation (6) comes from equation (5):\n",
    "\\begin{align}\n",
    "D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) \n",
    "& = \\int_D q(\\vartheta) \\ln \\frac{q(\\vartheta)}{p(\\vartheta | \\varphi)} d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\frac{q(\\vartheta)}{\\frac{p(\\vartheta, \\varphi)}{p(\\varphi)}} d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\left( \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} p(\\varphi) \\right) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\left( \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} \\right) + \\ln(p(\\varphi)) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\left( \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} \\right) d\\vartheta + \\int_D q(\\vartheta) \\ln(p(\\varphi)) d\\vartheta \\\\\n",
    "& = F + \\int_D q(\\vartheta) \\ln(p(\\varphi)) d\\vartheta \\\\\n",
    "& = F + \\ln(p(\\varphi)) \\int_D q(\\vartheta) d\\vartheta \\\\\n",
    "\\end{align}\n",
    "and since $q(\\vartheta)$ is a probability distribution, $\\int_D q(\\vartheta) d\\vartheta=1$, so:\n",
    "$$\n",
    "D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) = F + \\ln(p(\\varphi))\n",
    "$$\n",
    "\n",
    "Line 3 to line 4 above are true because $\\log(xy) = \\log(x) + \\log(y)$. Lines 4 to 5 and 5 to 6 are true because integration is a linear operator, i.e. $\\int f(x) + g(x) dx = \\int f(x) dx + \\int g(x) dx$ and $\\int a f(x) dx = a \\int f(x) dx$. We can move $\\ln(p(\\varphi))$ out of the integral because it does not contain the variable wrt which we are integrating, $\\vartheta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's inequality stuff\n",
    "\n",
    "- Equation 8 is justified by the Jensen's inequality. In probability theory, Jensen's inequality says that\n",
    "$$\n",
    "\\phi (E[X]) \\geq E[\\phi(X)]\n",
    "$$\n",
    "where $\\phi$ is a concave function, $X$ is a random variable, and $E[\\cdot]$ is the expectation operator. The expectation of some function $f(x)$ under a distribution $p$ is just $\\int_D p(x) f(x) dx$, so the Jensen's inequality is saying that:\n",
    "$$\n",
    "\\phi \\left( \\int_D p(x) f(x) dx \\right) \\geq \\int_D p(x) \\phi(f(x)) dx\n",
    "$$\n",
    "- It's actually not hard to see how the Jensen's inequality implies that the KL-divergence is non-negative (NOTE: the paper says \"greater than zero\", but that's wrong. It should be \"greater than or equal to 0\", which is written correctly in equation 8. KL-divergence is 0 iff the two distributions are identical except maybe for a set of measure 0). Here is the proof from a [SO question](https://math.stackexchange.com/questions/2031062/proof-of-nonnegativity-of-kl-divergence-using-jensens-inequality/2031944). Start with:\n",
    "$$\n",
    "KL(p||q) = -\\int p(x) \\log\\left(\\frac{q(x)}{p(x)}\\right)dx\n",
    "$$\n",
    "- This is just the definition of KL-divergence. Sometimes you see it written as $ KL(p||q) = \\int p(x) \\log\\left\\{\\frac{p(x)}{q(x)}\\right\\}dx$ (note that $p$ and $q$ are switched), but that's equivalent because $-\\log(\\frac{x}{y}) = \\log(\\frac{1}{\\frac{x}{y}})=\\log(\\frac{y}{x})$. Why $-\\log(\\frac{x}{y}) = \\log(\\frac{1}{\\frac{x}{y}})$? Because (1) $\\log(\\frac{x}{y})=\\log(x) - \\log(x)$, (2) $\\log(1) = 0$, and so $-\\log(\\frac{x}{y}) = 0 - \\log(\\frac{x}{y}) = \\log(1) - \\log(\\frac{x}{y}) = \\log(\\frac{1}{\\frac{x}{y}})$. \n",
    "- Anyway, $\\log$ is a concave function, so it can play the role of $\\phi$ in the statement of the Jensen's inequality above. Note that the KL-divergence is the negative expectation under $p$ of $\\log\\left(\\frac{q(x)}{p(x)}\\right)$. Take $f(x)$ above to be $\\frac{q(x)}{p(x)}$.\n",
    "\\begin{align}\n",
    "- KL(p||q) & = \\int p(x) \\log\\left(\\frac{q(x)}{p(x)}\\right)dx \\\\\n",
    "& \\le \\log \\int p(x) \\frac{q(x)}{p(x)}dx \\\\\n",
    "& = \\log \\int q(x)dx \\\\\n",
    "& = 0\n",
    "\\end{align}\n",
    "- The passage from line 1 to line 2 is just an application of Jensen's inequality. Then, simplify the $p(x)$. The last passage is because $\\int_D q(x)dx=1$, because $q(x)$ is a distribution, and $\\log(1)=0$. To conclude, since $-KL(p||q) \\le 0$, it follows that $KL(p||q) \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So how does 8 follow from $KL(p||q) \\ge 0$? Well, we also have to consider quation 6. From equation 6 follows that\n",
    "$$\n",
    "F = D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) - \\ln(p(\\varphi))\n",
    "$$\n",
    "- We know from Jensen's inequality that $D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi))$ is non-negative. Therefore, $F$ is equal to a non-negative value $A$ plus $-\\ln(p(\\varphi))$. Which means that $F$ is at least as big as $-\\ln(p(\\varphi))$ (and even bigger than that if $A \\not= 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, equation 9:\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) \n",
    "& = \\int_D q(\\vartheta) \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln q(\\vartheta) - \\ln p(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta - \\int_D q(\\vartheta) \\ln p(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) \\left( - \\ln p(\\vartheta, \\varphi) \\right) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\left( - \\ln p(\\vartheta, \\varphi) \\right) d\\vartheta + \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta \\\\\n",
    "\\end{align}\n",
    "- Then, they define\n",
    "$$\n",
    "E(\\vartheta, \\varphi) = -\\ln p(\\vartheta, \\varphi)\n",
    "$$\n",
    "(Unfortunately, they use $E$ which I was using for expected value because I can't be bothered to find the actual fancy symbol for expectation. But they don't mean it as expected value. Instead, they call it _energy_)\n",
    "- So rewriting the last equation above\n",
    "$$\n",
    "\\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta + \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computational_models] *",
   "language": "python",
   "name": "conda-env-computational_models-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
