{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A close, close reading of Buckley et al. (2017)\n",
    "\n",
    "\n",
    "## 1. Introduction and 2. An overview of FEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\{ \\vartheta \\}$: variable referring to the environmental states (world)\n",
    "- $\\{ \\varphi \\}$: variable referring to sensory inputs\n",
    "\n",
    "- $-\\ln( p ( \\varphi ) )$ is the surprisal of the _real_ distribution over sensory states. \n",
    "- Why this function? $-\\ln( p( x=a ) )$ is the _surprisal_ of random variable $x$ taking value $a$. The people who invented information theory picked the negative log because they wanted the measure of surprisal to respect certain conditions, which the log function does:\n",
    "\n",
    "1) Surprisal has to be 0 for an event that is certain (has probability 1)\n",
    "\n",
    "2) Surprisal has to be infinite for an impossible event (one that has probability 0. Here I am assuming a discrete distribution; things are more complicated with continuous distribution, e.g. events with prob 0 are not impossible). Isn't it annoying to deal with infinity? No, because you never actually observe an event with probability 0!\n",
    "\n",
    "3) The total surprisal of observing two independent events should be the sum of the surprisals of observing the individual events. (A bit more technically, why should surprisal be additive? Well, we decided above that a sure event $A$ has surprisal 0. Imagine observing a sure event in addition to an event $B$ with surprisal $b$. Clearly, the total surprisal doesn't increase by observing a sure event, but rather is the same as the one coming from just $B$. Therefore, you want the total surpisal to be the sum of the surprisals, i.e. $0+b=b$. This makes the additivity more intuitively plausible)\n",
    "\n",
    "- $\\{ \\varphi \\}$ is one of the reasons that free energy turns out to be so important: free energy gives us information about the real distribution over sensory states, which we need information about.\n",
    "\n",
    "Then there are two important densities that we have a model of in the brain:\n",
    "- R-density: $q(\\vartheta)$ density over world states. This is parameterized (see section 4).\n",
    "- G-density: $p(\\vartheta, \\varphi)$ joint density of world states and sensory input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variational free energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To start with, and just intuitively, why do we need to use a variational approximation of $p(\\vartheta | \\varphi)$, i.e. the posterior distribution over states after having gotten certain sensory inputs? The answer to this question is that to calculate the posterior we need to calculate the normalization constant $\\int_D p(\\varphi | \\vartheta) p(\\vartheta)$, which is hard to calculate. So instead of directly calculating the posterior, we calculate an approximation to the posterior, i.e. the R-density, and use the variational method to make the R-density close to the \"true\" posterior, which we cannot calculate.\n",
    "\n",
    "- Equation (1)\n",
    "$$\n",
    "p(\\vartheta, \\varphi) = p(\\varphi | \\vartheta) p(\\vartheta) \n",
    "$$\n",
    "is an instance of the _chain rule_ of probability.\n",
    "\n",
    "- All integrals are actually over the domain of the distribution, which is assumed to be $(-\\infty, \\infty)$. Moreover, they write the $dx$ right after the integral sign (because they are physicists I assume?), but I'll write it as the end because it functions like a closed parenthesis. So I'll write $\\int_D f(x) dx$ where they write $\\int f(x) dx$, unless they mean an indefinite integral.\n",
    "- Equation (6) comes from equation (5):\n",
    "\\begin{align}\n",
    "D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) \n",
    "& = \\int_D q(\\vartheta) \\ln \\frac{q(\\vartheta)}{p(\\vartheta | \\varphi)} d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\frac{q(\\vartheta)}{\\frac{p(\\vartheta, \\varphi)}{p(\\varphi)}} d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\left( \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} p(\\varphi) \\right) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\left( \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} \\right) + \\ln(p(\\varphi)) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln \\left( \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} \\right) d\\vartheta + \\int_D q(\\vartheta) \\ln(p(\\varphi)) d\\vartheta \\\\\n",
    "& = F + \\int_D q(\\vartheta) \\ln(p(\\varphi)) d\\vartheta \\\\\n",
    "& = F + \\ln(p(\\varphi)) \\int_D q(\\vartheta) d\\vartheta \\\\\n",
    "\\end{align}\n",
    "and since $q(\\vartheta)$ is a probability distribution, $\\int_D q(\\vartheta) d\\vartheta=1$, so:\n",
    "$$\n",
    "D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) = F + \\ln(p(\\varphi))\n",
    "$$\n",
    "\n",
    "Line 3 to line 4 above are true because $\\log(xy) = \\log(x) + \\log(y)$. Lines 4 to 5 and 5 to 6 are true because integration is a linear operator, i.e. $\\int f(x) + g(x) dx = \\int f(x) dx + \\int g(x) dx$ and $\\int a f(x) dx = a \\int f(x) dx$. We can move $\\ln(p(\\varphi))$ out of the integral because it does not contain the variable wrt which we are integrating, $\\vartheta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's inequality stuff\n",
    "\n",
    "- Equation 8 is justified by the Jensen's inequality. In probability theory, Jensen's inequality says that\n",
    "$$\n",
    "\\phi (E[X]) \\geq E[\\phi(X)]\n",
    "$$\n",
    "where $\\phi$ is a concave function, $X$ is a random variable, and $E[\\cdot]$ is the expectation operator. The expectation of some function $f(x)$ under a distribution $p$ is just $\\int_D p(x) f(x) dx$, so the Jensen's inequality is saying that:\n",
    "$$\n",
    "\\phi \\left( \\int_D p(x) f(x) dx \\right) \\geq \\int_D p(x) \\phi(f(x)) dx\n",
    "$$\n",
    "- It's actually not hard to see how the Jensen's inequality implies that the KL-divergence is non-negative (NOTE: the paper says \"greater than zero\", but that's wrong. It should be \"greater than or equal to 0\", which is written correctly in equation 8. KL-divergence is 0 iff the two distributions are identical except maybe for a set of measure 0). Here is the proof from a [SO question](https://math.stackexchange.com/questions/2031062/proof-of-nonnegativity-of-kl-divergence-using-jensens-inequality/2031944). Start with:\n",
    "$$\n",
    "KL(p||q) = -\\int p(x) \\log\\left(\\frac{q(x)}{p(x)}\\right)dx\n",
    "$$\n",
    "- This is just the definition of KL-divergence. Sometimes you see it written as $ KL(p||q) = \\int p(x) \\log\\left\\{\\frac{p(x)}{q(x)}\\right\\}dx$ (note that $p$ and $q$ are switched), but that's equivalent because $-\\log(\\frac{x}{y}) = \\log(\\frac{1}{\\frac{x}{y}})=\\log(\\frac{y}{x})$. Why $-\\log(\\frac{x}{y}) = \\log(\\frac{1}{\\frac{x}{y}})$? Because (1) $\\log(\\frac{x}{y})=\\log(x) - \\log(x)$, (2) $\\log(1) = 0$, and so $-\\log(\\frac{x}{y}) = 0 - \\log(\\frac{x}{y}) = \\log(1) - \\log(\\frac{x}{y}) = \\log(\\frac{1}{\\frac{x}{y}})$. \n",
    "- Anyway, $\\log$ is a concave function, so it can play the role of $\\phi$ in the statement of the Jensen's inequality above. Note that the KL-divergence is the negative expectation under $p$ of $\\log\\left(\\frac{q(x)}{p(x)}\\right)$. Take $f(x)$ above to be $\\frac{q(x)}{p(x)}$.\n",
    "\n",
    "\\begin{align}\n",
    "- KL(p||q) & = \\int p(x) \\log\\left(\\frac{q(x)}{p(x)}\\right)dx \\\\\n",
    "& \\le \\log \\int p(x) \\frac{q(x)}{p(x)}dx \\\\\n",
    "& = \\log \\int q(x)dx \\\\\n",
    "& = 0\n",
    "\\end{align}\n",
    "\n",
    "- The passage from line 1 to line 2 is just an application of Jensen's inequality. Then, simplify the $p(x)$. The last passage is because $\\int_D q(x)dx=1$, because $q(x)$ is a distribution, and $\\log(1)=0$. To conclude, since $-KL(p||q) \\le 0$, it follows that $KL(p||q) \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So how does 8 follow from $KL(p||q) \\ge 0$? Well, we also have to consider quation 6. From equation 6 follows that\n",
    "$$\n",
    "F = D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) - \\ln(p(\\varphi))\n",
    "$$\n",
    "- We know from Jensen's inequality that $D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi))$ is non-negative. Therefore, $F$ is equal to a non-negative value $A$ plus $-\\ln(p(\\varphi))$. Which means that $F$ is at least as big as $-\\ln(p(\\varphi))$ (and even bigger than that if $A \\not= 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, equation 9:\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL} (q(\\vartheta) || p(\\vartheta | \\varphi)) \n",
    "& = \\int_D q(\\vartheta) \\ln \\frac{q(\\vartheta)}{p(\\vartheta, \\varphi)} d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln q(\\vartheta) - \\ln p(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta - \\int_D q(\\vartheta) \\ln p(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) \\left( - \\ln p(\\vartheta, \\varphi) \\right) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\left( - \\ln p(\\vartheta, \\varphi) \\right) d\\vartheta + \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta \\\\\n",
    "\\end{align}\n",
    "- Then, they define\n",
    "$$\n",
    "E(\\vartheta, \\varphi) = -\\ln p(\\vartheta, \\varphi)\n",
    "$$\n",
    "(Unfortunately, they use $E$ which I was using for expected value because I can't be bothered to find the actual fancy symbol for expectation. But they don't mean it as expected value. Instead, they call it _energy_)\n",
    "- So rewriting the last equation above\n",
    "$$\n",
    "\\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta + \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The R-density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The idea here is that the brain (yes, the actual neurons!) somehow encode statistics of the distribution, i.e. instead of encoding the function from world-states to probabilities directly, they encode a functional form that specifies a distribution _family_ and then encode the R-density at any given moment by specifying the value of the parameters of the distribution family. This is called [_parametric_ statistics](https://en.wikipedia.org/wiki/Parametric_statistics).\n",
    "- For instance, they might encode the normal distribution family, which allows one to zoom in on a specific distribution by specifying a mean and a variance. \n",
    "\n",
    "- Now, to connect to the stuff in the previous section, assume that the world state is just a single real number. So maybe $\\vartheta = 0.4$, or maybe $\\vartheta = -100.3$, etc. The R-density $q(\\vartheta)$ is then just a distribution over real numbers. More specifically, they assume in the paper that the distribution $q$ is a normal distribution parameterized by a mean $\\mu$ and a variance $\\zeta$. \n",
    "- NOTE: they do something strange with the notation that has the potential to be very confusing. The first paragraph of section 4 they use the symbol $\\mu$ to refer to both a brain state and a parameter that parameterizes the R-density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now they introduce the idea of Laplace approximation. The idea is that you can't minimize the free energy in the most general case. \n",
    "\n",
    "So, for now there are two approximations: \n",
    "- The first is approximating the true posterior over world-states given perception with the R-distribution, which was done in section 3. This is done by finding the R-density that minimizes _the best we can_ the KL-divergence to the real posterior. We can't minimize the KL-divergence directly and totally (if we could bring the KL divergence to 0, we would have found the true posterior!), but instead we can optimize a part of it, which is called the free energy. The other bit is beyond out control, because it's the surpisal of the sensory states, which we don't know. This is what Hohwy says about this in his book _The Predictive Mind_ (p.52): \"If surprisal depends on sensory input, then we should be able to see perceptual inference as somehow a function relating to surprisal. What could that function be? Given that there is a sense in which surprisal is unexpected for the creature, and given we have talked about minimizing prediction error, perhaps the creature should directly minimize surprisal? This however is not possible. There is no way the creature can assess directly whether some particular state is surprising or not, to do that it would have to do the impossible task of averaging over an infinite number of copies of itself (under all possible hypotheses that could be entertained by the model) to see whether that is a state it is expected to be in or not\". By the way, I find this a bit confusing. I thought that we can't calculate surprisal because we don't have access to the objective normal distribution over sensory inputs for the type of creature that we are, NOT because we would have to do heavy computations (surveying possible world-states in our model). TODO: think more about this.\n",
    "- The second approximation is the one we have to deal with now. This is done because we can't even minimize the free energy part of the KL divergence perfectly. We can't do this because of computational reasons; we would have to consider all possible densities over world-states to find the one that minimizes free energy. But there is no way to do this search analytically! So instead we only look at a subset of all densities, namely the ones in the normal family. This is what the Laplace approximation does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does the Laplace approximation work? \n",
    "- First, we force our R-density to have a normal distribution:\n",
    "\n",
    "$$\n",
    "q(\\vartheta; \\mu, \\zeta) = N(\\vartheta | \\mu, \\zeta) = \\frac{1}{\\sqrt{2\\pi\\zeta}}\\exp( - \\frac{(\\vartheta - \\mu)^2}{2\\zeta}  )\n",
    "$$\n",
    "\n",
    "At this point they introduce two abbreviations:\n",
    "\n",
    "$$\n",
    "Z = \\sqrt{2\\pi\\zeta} \\\\\n",
    "\\varepsilon(\\vartheta) = \\frac{(\\vartheta - \\mu)^2}{2\\zeta}\n",
    "$$\n",
    "\n",
    "So that the whole normal distribution can be written as:\n",
    "\n",
    "$$\n",
    "q(\\vartheta; \\mu, \\zeta) = \\frac{1}{Z}e^{-\\varepsilon(\\vartheta)}\n",
    "$$\n",
    "\n",
    "- Second, we substitute this form of the R-density in equation (9), which was a way of writing free energy. Why do this? Well, remember that we are trying to minimize free energy. If we write free energy with the R-density as a normal distribution, and then minimize with respect to the $\\mu$ and $\\zeta$ parameters, we are finding the distribution among all the ones in the normal family that minimizes free energy. I rewrite equation 9 here:\n",
    "$$\n",
    "F = \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta + \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta\n",
    "$$\n",
    "They first switch the order of the addition (to make it more confusing?)\n",
    "$$\n",
    "F = \\int_D q(\\vartheta) \\ln q(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta\n",
    "$$\n",
    "and then they substitute the normal form of $q$:\n",
    "\\begin{align}\n",
    "F & = \\int_D q(\\vartheta) \\ln \\left( \\frac{1}{Z}e^{-\\varepsilon(\\vartheta)} \\right) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\left( \\ln ( \\frac{1}{Z} ) + \\ln ( e^{-\\varepsilon(\\vartheta)} ) \\right) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\left( \\ln ( Z^{-1} ) + (-\\varepsilon(\\vartheta)) \\ln ( e ) \\right) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D q(\\vartheta) \\left( - \\ln ( Z ) -\\varepsilon(\\vartheta)\\right) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lines 1$\\to$2 are because, as mentioned above, $\\log(ab) = \\log(a) + \\log(b)$. Lines 2$\\to$3 are because $\\log(a^b) = b \\log (a)$. Note further that $\\frac{1}{b} = b^{-1}$ and that $\\ln(e)=1$. We continue like this:\n",
    "\n",
    "\\begin{align}\n",
    "& = \\int_D - q(\\vartheta) \\ln ( Z ) - q(\\vartheta) \\varepsilon(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = \\int_D - q(\\vartheta) \\ln ( Z ) d\\vartheta - \\int_D q(\\vartheta) \\varepsilon(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = - \\ln( Z ) \\int_D q(\\vartheta) d\\vartheta - \\int_D q(\\vartheta) \\varepsilon(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "& = - \\ln( Z ) - \\int_D q(\\vartheta) \\varepsilon(\\vartheta) d\\vartheta + \\int_D q(\\vartheta) E(\\vartheta, \\varphi) d\\vartheta \\\\\n",
    "\\end{align}\n",
    "- We just applied rules explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall the definitions of $Z$ and $\\varepsilon$ above. Now consider the addition in the last line term by term, starting with the first term:\n",
    "$$\n",
    "-\\ln(Z) = - \\ln( \\sqrt{2\\pi\\zeta} ) = - \\ln( (2\\pi\\zeta)^{\\frac{1}{2}} ) = - \\frac{1}{2} \\ln( 2\\pi\\zeta )\n",
    "$$\n",
    "\n",
    "- Now the second term:\n",
    "$$\n",
    "- \\int_D q(\\vartheta) \\varepsilon(\\vartheta) d\\vartheta = \\\\\n",
    "- \\int_D q(\\vartheta) \\frac{(\\vartheta - \\mu)^2}{2\\zeta} d\\vartheta = \\\\\n",
    "- \\frac{1}{2\\zeta} \\int_D q(\\vartheta) (\\vartheta - \\mu)^2 d\\vartheta \n",
    "$$\n",
    "Notice that $\\int_D q(\\vartheta) (\\vartheta - \\mu)^2 d\\vartheta$ is the definition of the second central moment, i.e. the variance, which we called $\\zeta$. So:\n",
    "$$\n",
    "- \\frac{1}{2\\zeta} \\zeta = -\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "- Finally, the third term, which includes the energy $E = -\\ln p(\\vartheta, \\varphi)$. The third term is where the approximation, really happens, because instead of using $E$, they use an approximation of $E$ obtained with a Taylor expansion around the mean $\\mu$. \n",
    "- Explaining what a Taylor expansion is would take too much space here, so instead I am just gonna link [this amazing video](https://www.youtube.com/watch?v=3d6DsjIBzJ4) that does a better job at explaining than I could. But basically the idea is that we can approximate $E$ as follows:\n",
    "$$\n",
    "E(\\vartheta, \\varphi) \\approx E(\\mu, \\varphi) + \\left[ \\frac{\\partial E}{\\partial \\vartheta} \\right]_\\mu (\\vartheta - \\mu) + \\frac{1}{2} \\left[ \\frac{\\partial^2 E}{\\partial \\vartheta^2} \\right]_\\mu (\\vartheta - \\mu)^2\n",
    "$$\n",
    "- Note that the $E$ function and its derivatives are always evaluate at $\\mu$ on the right side. Moreover, the notation they use $\\left[ \\frac{\\partial f}{\\partial x} \\right]_a$ means to evaluate the derivative at $a$. They write $\\vartheta - \\mu$ as $\\delta\\vartheta$, but I think this is not great because it obscures the form of the Taylor approximation for people who are not familiar with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the third term is approximately equal to:\n",
    "\n",
    "$$\n",
    "\\int_D q(\\vartheta) \\left( E(\\mu, \\varphi) + \\left[ \\frac{\\partial E}{\\partial \\vartheta} \\right]_\\mu (\\vartheta - \\mu) + \\frac{1}{2} \\left[ \\frac{\\partial^2 E}{\\partial \\vartheta^2} \\right]_\\mu (\\vartheta - \\mu)^2 \\right) d\\vartheta = \\\\\n",
    "\\int_D q(\\vartheta) E(\\mu, \\varphi) d\\vartheta + \\int_D q(\\vartheta) \\left[ \\frac{\\partial E}{\\partial \\vartheta} \\right]_\\mu (\\vartheta - \\mu) d\\vartheta + \\int_D q(\\vartheta) \\frac{1}{2} \\left[ \\frac{\\partial^2 E}{\\partial \\vartheta^2} \\right]_\\mu (\\vartheta - \\mu)^2 d\\vartheta = \\\\\n",
    "E(\\mu, \\varphi) \\int_D q(\\vartheta) d\\vartheta + \\left[ \\frac{\\partial E}{\\partial \\vartheta} \\right]_\\mu \\int_D q(\\vartheta) (\\vartheta - \\mu) d\\vartheta + \\frac{1}{2}  \\left[ \\frac{\\partial^2 E}{\\partial \\vartheta^2} \\right]_\\mu \\int_D q(\\vartheta) (\\vartheta - \\mu)^2 d\\vartheta\n",
    "$$\n",
    "- Note that we can move the derivatives outside of the integral because they are evaluated at $\\mu$, and so are not a function of $\\zeta$ anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To simplify further, notice that $\\int_D q(\\vartheta) d\\vartheta = 1$, because $q$ is a distribution. Moreover, $\\left[ \\frac{\\partial E}{\\partial \\vartheta} \\right]_\\mu \\int_D q(\\vartheta) (\\vartheta - \\mu) d\\vartheta = 0$. This is because for any distribution $f$ with support $D$, $\\int_D f(x) (x - y) dx = \\int_D f(x) x dx - \\int_D f(x) y dx = \\mu_f - \\int_D f(x) y dx = \\mu_f - y \\int_D f(x)dx = \\mu_f - y$. So $\\int_D f(x) (x - \\mu_f) dx = 0$. And $\\mu$ is indeed the mean of $q$.\n",
    "- Finally, as already discussed, $\\int_D q(\\vartheta) (\\vartheta - \\mu)^2 d\\vartheta$ is the variance of the distribution, which we called $\\zeta$. So the whole expression above becomes equation 15:\n",
    "$$\n",
    "E(\\mu, \\varphi) + \\frac{1}{2}  \\left[ \\frac{\\partial^2 E}{\\partial \\vartheta^2} \\right]_\\mu \\zeta\n",
    "$$\n",
    "- This is the final approximation of the third term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So putting together all of these approximations, we get to equation 16:\n",
    "\n",
    "$$\n",
    "F = E(\\mu, \\varphi) + \\frac{1}{2} \\left( \\left[ \\frac{\\partial^2 E}{\\partial \\vartheta^2} \\right]_\\mu \\zeta -\\ln 2\\pi\\vartheta -1 \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
